from pyspark import SparkContext, SparkConf, StorageLevel
import time

conf = SparkConf().setAppName("Lab2-Persistence").setMaster("local[*]")

sc = SparkContext(conf=conf)

print("=" * 80)
print("LAB 2: RDD PERSISTENCE AND CACHING STRATEGIES")
print("=" * 80)

# =====================================================
# PART 1: Understanding the Problem - No Caching
# =====================================================
print("\n[PART 1] Baseline - No Caching\n")

# Create expensive computation
def expensive_transformation(x):
    """Simulate expensive computation"""
    result = x
    for _ in range(1000):
        result = (result * 1.1 + 0.5) / 1.05
    return result

# Large dataset
print("Creating large dataset...")
large_data = sc.parallelize(range(1, 100001), numSlices=8)  # 100k elements

# Apply expensive transformation
print("Applying expensive transformation...")
processed_rdd = large_data.map(expensive_transformation)

# Multiple actions on same RDD (without caching)
print("\nExecuting multiple actions WITHOUT caching:")

start = time.time()
count1 = processed_rdd.count()
time1 = time.time() - start
print(f"  Action 1 (count):  {time1:.3f}s - Count: {count1}")

start = time.time()
sum_result = processed_rdd.reduce(lambda a, b: a + b)
time2 = time.time() - start
print(f"  Action 2 (sum):    {time2:.3f}s - Sum: {sum_result:.2f}")

start = time.time()
max_val = processed_rdd.max()
time3 = time.time() - start
print(f"  Action 3 (max):    {time3:.3f}s - Max: {max_val:.2f}")

total_time_no_cache = time1 + time2 + time3
print(f"\n  Total time: {total_time_no_cache:.3f}s")
print("  ‚ö†Ô∏è  Each action recomputes the entire RDD from scratch!")

# =====================================================
# PART 2: Using cache() - Memory Storage
# =====================================================
print("\n[PART 2] Using cache() - Memory Storage\n")

# Recreate RDD and apply caching
large_data_cached = sc.parallelize(range(1, 100001), numSlices=8)
processed_cached = large_data_cached.map(expensive_transformation)

# Cache in memory
print("Calling cache() on RDD...")
processed_cached.cache()

print("\nExecuting multiple actions WITH cache():")

start = time.time()
count1 = processed_cached.count()  # Triggers computation + caching
time1 = time.time() - start
print(f"  Action 1 (count):  {time1:.3f}s - Count: {count1} [COMPUTED + CACHED]")

start = time.time()
sum_result = processed_cached.reduce(lambda a, b: a + b)  # Uses cache
time2 = time.time() - start
print(f"  Action 2 (sum):    {time2:.3f}s - Sum: {sum_result:.2f} [FROM CACHE]")

start = time.time()
max_val = processed_cached.max()  # Uses cache
time3 = time.time() - start
print(f"  Action 3 (max):    {time3:.3f}s - Max: {max_val:.2f} [FROM CACHE]")

total_time_cached = time1 + time2 + time3
print(f"\n  Total time: {total_time_cached:.3f}s")

improvement = ((total_time_no_cache - total_time_cached) / total_time_no_cache) * 100
print(f"  ‚ö° Performance improvement: {improvement:.1f}%")

# =====================================================
# PART 3: Different Storage Levels
# =====================================================
print("\n[PART 3] Comparing Different Storage Levels\n")

storage_levels = [
    ("MEMORY_ONLY", StorageLevel.MEMORY_ONLY),
    ("MEMORY_AND_DISK", StorageLevel.MEMORY_AND_DISK),
    ("MEMORY_ONLY_SER", StorageLevel.MEMORY_ONLY_SER),
    ("DISK_ONLY", StorageLevel.DISK_ONLY)
]

print(f"{'Storage Level':<20} {'First Action':<15} {'Second Action':<15} {'Total':<10}")
print("-" * 65)

comparison_results = []

for name, level in storage_levels:
    # Create fresh RDD
    test_data = sc.parallelize(range(1, 100001), numSlices=8)
    test_rdd = test_data.map(expensive_transformation)
    
    # Persist with specific storage level
    test_rdd.persist(level)
    
    # First action (computation + storage)
    start = time.time()
    test_rdd.count()
    time_first = time.time() - start
    
    # Second action (from storage)
    start = time.time()
    test_rdd.reduce(lambda a, b: a + b)
    time_second = time.time() - start
    
    total = time_first + time_second
    print(f"{name:<20} {time_first:<15.3f} {time_second:<15.3f} {total:<10.3f}")
    
    comparison_results.append((name, time_first, time_second, total))
    
    # Cleanup
    test_rdd.unpersist()

# Find best performance
best = min(comparison_results, key=lambda x: x[3])
print(f"\n‚ö° Best performance: {best[0]} (Total: {best[3]:.3f}s)")

# =====================================================
# PART 4: Storage Level Characteristics
# =====================================================
print("\n[PART 4] Storage Level Characteristics\n")

print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Storage Level       ‚îÇ Memory   ‚îÇ Disk     ‚îÇ Serialized  ‚îÇ Recompute    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ MEMORY_ONLY         ‚îÇ Yes      ‚îÇ No       ‚îÇ No          ‚îÇ On eviction  ‚îÇ
‚îÇ MEMORY_AND_DISK     ‚îÇ Yes      ‚îÇ Spill    ‚îÇ No          ‚îÇ No           ‚îÇ
‚îÇ MEMORY_ONLY_SER     ‚îÇ Yes      ‚îÇ No       ‚îÇ Yes         ‚îÇ On eviction  ‚îÇ
‚îÇ MEMORY_AND_DISK_SER ‚îÇ Yes      ‚îÇ Spill    ‚îÇ Yes         ‚îÇ No           ‚îÇ
‚îÇ DISK_ONLY           ‚îÇ No       ‚îÇ Yes      ‚îÇ Yes         ‚îÇ No           ‚îÇ
‚îÇ OFF_HEAP            ‚îÇ Off-heap ‚îÇ No       ‚îÇ Yes         ‚îÇ On eviction  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Observations:
""")

for name, time_first, time_second, total in comparison_results:
    speedup = time_first / time_second if time_second > 0 else 0
    print(f"\n{name}:")
    print(f"  ‚Ä¢ First action:  {time_first:.3f}s (compute + store)")
    print(f"  ‚Ä¢ Second action: {time_second:.3f}s (retrieve)")
    print(f"  ‚Ä¢ Speedup factor: {speedup:.1f}x on cached actions")

# =====================================================
# PART 5: When to Use Each Storage Level
# =====================================================
print("\n[PART 5] When to Use Each Storage Level\n")

print("""
MEMORY_ONLY (Default for cache()):
  ‚úì Use when: Dataset fits in memory
  ‚úì Best for: Fast access, iterative algorithms
  ‚úó Risk: Data loss if memory pressure
  üìä Example: Small to medium datasets, ML training

MEMORY_AND_DISK:
  ‚úì Use when: Dataset might not fit in memory
  ‚úì Best for: Production jobs, critical data
  ‚úó Tradeoff: Slower disk access vs recomputation
  üìä Example: Large datasets, long computations

MEMORY_ONLY_SER (Serialized):
  ‚úì Use when: Memory is limited but CPU is available
  ‚úì Best for: Space-efficient storage
  ‚úó Tradeoff: CPU overhead for serialization/deserialization
  üìä Example: Large objects, memory-constrained environments

MEMORY_AND_DISK_SER:
  ‚úì Use when: Need space efficiency + reliability
  ‚úì Best for: Large production jobs
  ‚úó Tradeoff: CPU overhead + potential disk I/O
  üìä Example: Enterprise ETL jobs

DISK_ONLY:
  ‚úì Use when: Data is too large for memory
  ‚úì Best for: Archival, very large intermediate results
  ‚úó Tradeoff: Slowest access, but no memory used
  üìä Example: Data preparation, large shuffles

OFF_HEAP:
  ‚úì Use when: Need precise memory management
  ‚úì Best for: Large cached data, reduce GC pressure
  ‚úó Tradeoff: Requires configuration, serialization overhead
  üìä Example: Long-running applications, large caches
""")

# =====================================================
# PART 6: Cache vs Persist - Practical Comparison
# =====================================================
print("\n[PART 6] cache() vs persist() - What's the Difference?\n")

print("""
cache() vs persist():

cache():
  ‚Ä¢ Shorthand for persist(StorageLevel.MEMORY_ONLY)
  ‚Ä¢ Simple API for common use case
  ‚Ä¢ Example: rdd.cache()

persist(storageLevel):
  ‚Ä¢ Explicit storage level control
  ‚Ä¢ More flexibility for optimization
  ‚Ä¢ Example: rdd.persist(StorageLevel.MEMORY_AND_DISK)

RECOMMENDATION:
  ‚Ä¢ Use cache() for quick prototyping and small datasets
  ‚Ä¢ Use persist() with specific level for production code
  ‚Ä¢ Always consider memory availability and access patterns
""")

# =====================================================
# PART 7: Real-World Scenarios
# =====================================================
print("\n[PART 7] Real-World Scenarios and Recommendations\n")

scenarios = [
    {
        "name": "Iterative Machine Learning",
        "description": "Training model with multiple passes over data",
        "recommendation": "MEMORY_ONLY or MEMORY_AND_DISK",
        "reason": "Multiple iterations benefit from fast memory access"
    },
    {
        "name": "ETL Pipeline with Multiple Outputs",
        "description": "Process data once, write to multiple destinations",
        "recommendation": "MEMORY_AND_DISK",
        "reason": "Ensures data persists through multiple actions"
    },
    {
        "name": "Exploratory Data Analysis",
        "description": "Interactive queries on same dataset",
        "recommendation": "MEMORY_ONLY or MEMORY_ONLY_SER",
        "reason": "Fast interactive response times"
    },
    {
        "name": "Large-Scale Log Processing",
        "description": "Process TB-scale logs with aggregations",
        "recommendation": "MEMORY_AND_DISK_SER or DISK_ONLY",
        "reason": "Data too large for memory, need fault tolerance"
    },
    {
        "name": "Stream Processing with Windowing",
        "description": "Maintain windows of recent data",
        "recommendation": "MEMORY_AND_DISK",
        "reason": "Balance speed and reliability for real-time"
    }
]

for i, scenario in enumerate(scenarios, 1):
    print(f"\nScenario {i}: {scenario['name']}")
    print(f"  Description: {scenario['description']}")
    print(f"  ‚úì Recommended: {scenario['recommendation']}")
    print(f"  üí° Reason: {scenario['reason']}")

# =====================================================
# PART 8: Best Practices and Pitfalls
# =====================================================
print("\n[PART 8] Best Practices and Common Pitfalls\n")

print("""
BEST PRACTICES:
‚úì Cache only RDDs that are reused multiple times
‚úì Call cache() before the first action (it's lazy)
‚úì Unpersist RDDs when done to free memory
‚úì Monitor memory usage in Spark UI
‚úì Use appropriate storage level for your use case
‚úì Consider serialization cost vs memory savings

COMMON PITFALLS:
‚úó Caching everything (memory waste)
‚úó Caching RDDs used only once (no benefit)
‚úó Not unpersisting unused cached RDDs
‚úó Using MEMORY_ONLY for large datasets (eviction thrashing)
‚úó Ignoring Spark UI memory metrics
‚úó Over-serialization (CPU overhead)

MEMORY MANAGEMENT TIPS:
‚Ä¢ spark.memory.fraction (default 0.6): Execution + storage
‚Ä¢ spark.memory.storageFraction (default 0.5): Storage within memory
‚Ä¢ Monitor "Storage" tab in Spark UI
‚Ä¢ Use rdd.getStorageLevel() to check current level
‚Ä¢ Check "Blocked" memory in UI for evictions
""")

# =====================================================
# PART 9: Practical Exercise
# =====================================================
print("\n[PART 9] Practical Exercise - Finding Optimal Strategy\n")

# Simulate a multi-stage pipeline
print("Simulating multi-stage data pipeline...\n")

# Stage 1: Load and clean
raw_data = sc.parallelize(range(1, 50001), numSlices=8)
cleaned = raw_data.map(lambda x: x * 2).filter(lambda x: x % 3 != 0)

# Stage 2: Enrich (expensive)
enriched = cleaned.map(expensive_transformation)

# Stage 3: Aggregate (multiple actions)
print("Testing different caching strategies:")

strategies = [
    ("No caching", None),
    ("Cache cleaned", "cleaned"),
    ("Cache enriched", "enriched"),
    ("Cache both", "both")
]

for strategy_name, cache_point in strategies:
    # Recreate pipeline
    raw = sc.parallelize(range(1, 50001), numSlices=8)
    clean = raw.map(lambda x: x * 2).filter(lambda x: x % 3 != 0)
    enrich = clean.map(expensive_transformation)
    
    # Apply caching strategy
    if cache_point == "cleaned":
        clean.cache()
    elif cache_point == "enriched":
        enrich.cache()
    elif cache_point == "both":
        clean.cache()
        enrich.cache()
    
    # Measure performance
    start = time.time()
    count = enrich.count()
    sum_val = enrich.reduce(lambda a, b: a + b)
    max_val = enrich.max()
    total_time = time.time() - start
    
    print(f"  {strategy_name:20s}: {total_time:.3f}s")
    
    # Cleanup
    clean.unpersist()
    enrich.unpersist()

print("\nüí° Observation: Cache at the point where computation becomes expensive")
print("   and the data will be reused multiple times.")

# =====================================================
# SUMMARY
# =====================================================
print("\n" + "=" * 80)
print("KEY TAKEAWAYS")
print("=" * 80)
print(f"""
PERFORMANCE SUMMARY:
‚Ä¢ Without caching: {total_time_no_cache:.3f}s (baseline)
‚Ä¢ With caching:    {total_time_cached:.3f}s
‚Ä¢ Improvement:     {improvement:.1f}%

WHEN TO CACHE:
‚úì RDD is used 2+ times in your application
‚úì Computation is expensive (complex transformations)
‚úì Dataset fits in available memory (or use MEMORY_AND_DISK)
‚úì Iterative algorithms (ML, graph processing)

WHEN NOT TO CACHE:
‚úó RDD is used only once
‚úó Data is too large and causes memory pressure
‚úó Simple transformations (faster to recompute)
‚úó Streaming with non-overlapping windows

STORAGE LEVEL DECISION TREE:
1. Will RDD be reused? ‚Üí No: Don't cache
2. Dataset size vs available memory?
   ‚Üí Fits easily: MEMORY_ONLY
   ‚Üí Might not fit: MEMORY_AND_DISK
   ‚Üí Definitely too large: DISK_ONLY or recompute
3. Is serialization worthwhile?
   ‚Üí Large objects: Yes, use *_SER variant
   ‚Üí Small objects: No, overhead not worth it

MONITORING:
‚Ä¢ Spark UI ‚Üí Storage tab: See cached RDDs
‚Ä¢ Check RDD size and percentage cached
‚Ä¢ Monitor memory usage and evictions
‚Ä¢ Use rdd.toDebugString() to see lineage
""")

# Cleanup
processed_cached.unpersist()

sc.stop()